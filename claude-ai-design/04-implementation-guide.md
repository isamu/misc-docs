# Claudeé¢¨ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€OSSãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦Claudeé¢¨ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã™ã‚‹å…·ä½“çš„ãªæ‰‹é †ã‚’è§£èª¬ã—ã¾ã™ã€‚

---

## ğŸ› ï¸ å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã‚¹ã‚¿ãƒƒã‚¯

### æ¨å¥¨æ§‹æˆ

| ã‚«ãƒ†ã‚´ãƒª | ãƒ„ãƒ¼ãƒ« | ç”¨é€” | ç†ç”± |
|---------|-------|------|------|
| **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ** | LangGraph | çŠ¶æ…‹æ©Ÿæ¢°ãƒ»å®Ÿè¡Œãƒ«ãƒ¼ãƒ— | Anthropicæ¨å¥¨ |
| **LLMæ¥ç¶š** | LangChain | LLMæŠ½è±¡åŒ– | ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ãŒè±Šå¯Œ |
| **ãƒ„ãƒ¼ãƒ«æ¥ç¶š** | Model Context Protocol (MCP) | çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ | Anthropicå…¬å¼ |
| **ãƒ™ã‚¯ãƒˆãƒ«DB** | Chroma / Qdrant | ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ | è»½é‡ãƒ»é«˜é€Ÿ |
| **æ¨è«–ã‚µãƒ¼ãƒãƒ¼** | vLLM / Ollama | ãƒ­ãƒ¼ã‚«ãƒ«LLM | ä½ã‚³ã‚¹ãƒˆ |

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Pythonç’°å¢ƒï¼ˆæ¨å¥¨: 3.11+ï¼‰
pip install langgraph langchain langchain-anthropic
pip install chromadb qdrant-client
pip install tiktoken  # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆ

# Model Context Protocol SDK
npm install @modelcontextprotocol/sdk

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
pip install docling unstructured

# å‹•ç”»å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install llava-next qwen-vl
```

---

## ğŸ—ï¸ ã‚¹ãƒ†ãƒƒãƒ—1: Stateã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©

LangGraphã®StateGraphã‚’ä½¿ã„ã¾ã™ã€‚

### state.py

```python
from typing import TypedDict, List, Annotated
from langgraph.graph import add_messages

class Fact(TypedDict):
    content: str
    source: str  # 'user' | 'tool' | 'inference'
    confidence: str  # 'high' | 'medium' | 'low'
    timestamp: float

class Question(TypedDict):
    question: str
    priority: str  # 'high' | 'medium' | 'low'
    blocks_progress: bool

class PlanStep(TypedDict):
    id: str
    description: str
    status: str  # 'pending' | 'in_progress' | 'completed' | 'failed'
    dependencies: List[str]
    result: str | None

class AgentState(TypedDict):
    # ç›®æ¨™
    goal: str
    original_goal: str

    # çŸ¥è­˜
    known_facts: List[Fact]
    open_questions: List[Question]

    # è¨ˆç”»
    plan_steps: List[PlanStep]
    current_step_index: int

    # ä¼šè©±å±¥æ­´ï¼ˆLangGraphã®çµ„ã¿è¾¼ã¿ï¼‰
    messages: Annotated[list, add_messages]

    # è¦³æ¸¬
    observations: List[dict]

    # ãƒ¡ã‚¿æƒ…å ±
    iteration_count: int
    status: str  # 'planning' | 'executing' | 'blocked' | 'completed'
```

---

## ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—2: Plan-Act-Observe-Reflectã®å®Ÿè£…

LangGraphã§ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’å®šç¾©ã—ã¾ã™ã€‚

### agent.py

```python
from langgraph.graph import StateGraph, END
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# LLMã®åˆæœŸåŒ–
llm = ChatAnthropic(
    model="claude-opus-4-5-20251101",
    temperature=0
)

# ãƒãƒ¼ãƒ‰å®šç¾©

def plan_node(state: AgentState) -> AgentState:
    """è¨ˆç”»ãƒ•ã‚§ãƒ¼ã‚º"""
    print(f"[PLAN] Planning for goal: {state['goal']}")

    # Context Builderã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
    context = build_planning_context(state)

    # LLMã«è¨ˆç”»ã‚’ä¾é ¼
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=context)
    ]

    response = llm.invoke(messages)
    plan_steps = parse_plan(response.content)

    return {
        **state,
        "plan_steps": plan_steps,
        "current_step_index": 0,
        "status": "executing",
        "messages": state["messages"] + [response]
    }

def act_node(state: AgentState) -> AgentState:
    """å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º"""
    current_step = state["plan_steps"][state["current_step_index"]]
    print(f"[ACT] Executing step: {current_step['description']}")

    # ãƒ„ãƒ¼ãƒ«å¿…è¦æ€§ã®åˆ¤æ–­
    context = build_action_context(state, current_step)

    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=context)
    ]

    response = llm.invoke(messages)
    action = parse_action(response.content)

    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    new_state = state.copy()

    if action["type"] == "use_tool":
        # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
        tool_result = execute_tool(action["tool"], action["params"])

        new_state["observations"].append({
            "source": action["tool"],
            "content": tool_result,
            "timestamp": time.time()
        })

    new_state["messages"] = state["messages"] + [response]

    return new_state

def observe_node(state: AgentState) -> AgentState:
    """è¦³å¯Ÿãƒ•ã‚§ãƒ¼ã‚º"""
    print("[OBSERVE] Analyzing observations")

    if not state["observations"]:
        return state

    latest_obs = state["observations"][-1]

    # è¦³æ¸¬çµæœã®è§£é‡ˆ
    context = build_observation_context(state, latest_obs)

    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=context)
    ]

    response = llm.invoke(messages)
    interpretation = parse_interpretation(response.content)

    # æ–°ã—ã„äº‹å®Ÿã‚’è¿½åŠ 
    new_facts = state["known_facts"] + interpretation["new_facts"]

    # ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
    plan_steps = state["plan_steps"].copy()
    if interpretation["step_impact"] == "completed":
        plan_steps[state["current_step_index"]]["status"] = "completed"

    return {
        **state,
        "known_facts": new_facts,
        "plan_steps": plan_steps,
        "messages": state["messages"] + [response]
    }

def reflect_node(state: AgentState) -> AgentState:
    """åçœãƒ•ã‚§ãƒ¼ã‚º"""
    print("[REFLECT] Evaluating progress")

    context = build_reflection_context(state)

    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=context)
    ]

    response = llm.invoke(messages)
    reflection = parse_reflection(response.content)

    new_state = state.copy()
    new_state["iteration_count"] += 1
    new_state["messages"] = state["messages"] + [response]

    # æ¬¡ã®çŠ¶æ…‹ã‚’æ±ºå®š
    if reflection["goal_achieved"]:
        new_state["status"] = "completed"
    elif reflection["needs_replanning"]:
        new_state["status"] = "planning"
        new_state["plan_steps"] = []
    elif reflection["needs_user_input"]:
        new_state["status"] = "blocked"
    else:
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸
        new_state["current_step_index"] += 1
        if new_state["current_step_index"] >= len(state["plan_steps"]):
            new_state["status"] = "completed"

    return new_state

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°

def should_continue(state: AgentState) -> str:
    """æ¬¡ã®ãƒãƒ¼ãƒ‰ã‚’æ±ºå®š"""
    status = state["status"]

    if status == "completed":
        return "end"
    elif status == "planning":
        return "plan"
    elif status == "blocked":
        return "wait_user"
    elif status == "executing":
        return "act"
    else:
        return "end"

def after_reflect(state: AgentState) -> str:
    """Reflectå¾Œã®é·ç§»"""
    if state["status"] == "completed":
        return "end"
    elif state["status"] == "planning":
        return "plan"
    elif state["status"] == "blocked":
        return "wait_user"
    else:
        return "act"

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰

workflow = StateGraph(AgentState)

# ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
workflow.add_node("plan", plan_node)
workflow.add_node("act", act_node)
workflow.add_node("observe", observe_node)
workflow.add_node("reflect", reflect_node)

# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
workflow.set_entry_point("plan")

# ã‚¨ãƒƒã‚¸
workflow.add_edge("plan", "act")
workflow.add_edge("act", "observe")
workflow.add_edge("observe", "reflect")

# æ¡ä»¶ä»˜ãã‚¨ãƒƒã‚¸
workflow.add_conditional_edges(
    "reflect",
    after_reflect,
    {
        "plan": "plan",
        "act": "act",
        "wait_user": END,  # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å¾…ã¡
        "end": END
    }
)

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
app = workflow.compile()
```

---

## ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—3: Context Builderã®å®Ÿè£…

### context_builder.py

```python
from typing import List, Dict
import tiktoken

class ContextBuilder:
    def __init__(self, max_tokens: int = 180000):
        self.max_tokens = max_tokens
        self.encoder = tiktoken.get_encoding("o200k_base")  # Claudeç”¨

    def build_planning_context(self, state: AgentState) -> str:
        """è¨ˆç”»ãƒ•ã‚§ãƒ¼ã‚ºç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
        context = f"""
<goal>
{state['goal']}
</goal>

<known_facts>
{self._format_facts(state['known_facts'])}
</known_facts>

<open_questions>
{self._format_questions(state['open_questions'])}
</open_questions>

ã‚¿ã‚¹ã‚¯: ä¸Šè¨˜ã®ç›®æ¨™ã‚’é”æˆã™ã‚‹ãŸã‚ã®è©³ç´°ãªè¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚

è¨ˆç”»å½¢å¼:
<plan>
  <strategy>å…¨ä½“æˆ¦ç•¥</strategy>
  <steps>
    <step id="1">ã‚¹ãƒ†ãƒƒãƒ—1</step>
    <step id="2" depends_on="1">ã‚¹ãƒ†ãƒƒãƒ—2</step>
  </steps>
</plan>
"""
        return self._compress_if_needed(context)

    def build_action_context(
        self,
        state: AgentState,
        current_step: PlanStep
    ) -> str:
        """å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚ºç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
        context = f"""
<current_step>
{current_step['description']}
</current_step>

<recent_observations>
{self._format_recent_observations(state['observations'], max_count=5)}
</recent_observations>

<available_tools>
{self._format_tools(AVAILABLE_TOOLS)}
</available_tools>

ã‚¿ã‚¹ã‚¯: ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”:
<action type="use_tool|no_tool|ask_user">
  <tool>ãƒ„ãƒ¼ãƒ«åï¼ˆuse_toolã®å ´åˆï¼‰</tool>
  <params>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿JSON</params>
  <reason>ç†ç”±</reason>
</action>
"""
        return self._compress_if_needed(context)

    def _format_facts(self, facts: List[Fact]) -> str:
        """äº‹å®Ÿã‚’XMLå½¢å¼ã§æ•´å½¢"""
        if not facts:
            return "<none />"

        return "\n".join([
            f'<fact source="{f["source"]}" confidence="{f["confidence"]}">'
            f'{f["content"]}'
            f'</fact>'
            for f in facts
        ])

    def _format_observations(
        self,
        observations: List[dict],
        max_count: int = 10
    ) -> str:
        """è¦³æ¸¬ã‚’XMLå½¢å¼ã§æ•´å½¢"""
        recent = observations[-max_count:]

        if not recent:
            return "<none />"

        return "\n".join([
            f'<observation source="{o["source"]}">'
            f'{o["content"]}'
            f'</observation>'
            for o in recent
        ])

    def _compress_if_needed(self, context: str) -> str:
        """å¿…è¦ã«å¿œã˜ã¦åœ§ç¸®"""
        token_count = len(self.encoder.encode(context))

        if token_count > self.max_tokens * 0.8:
            # åœ§ç¸®æˆ¦ç•¥ã‚’é©ç”¨
            # ä¾‹: å¤ã„è¦³æ¸¬ã‚’å‰Šé™¤ã€è¦ç´„ãªã©
            pass

        return context

    def count_tokens(self, text: str) -> int:
        """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        return len(self.encoder.encode(text))
```

---

## ğŸ”Œ ã‚¹ãƒ†ãƒƒãƒ—4: Model Context Protocol (MCP)çµ±åˆ

MCPã‚’ä½¿ã£ã¦ãƒ„ãƒ¼ãƒ«ã‚’æ¨™æº–åŒ–ã—ã¾ã™ã€‚

### mcp_tools.py

```python
from mcp import Server, Tool
from mcp.server.stdio import stdio_server

# MCPã‚µãƒ¼ãƒãƒ¼ã®å®šç¾©

app = Server("claude-style-agent")

@app.list_tools()
async def list_tools() -> list[Tool]:
    """åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ"""
    return [
        Tool(
            name="web_search",
            description="Search the web for information",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Search query"
                    },
                    "max_results": {
                        "type": "number",
                        "description": "Max results",
                        "default": 5
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="read_file",
            description="Read contents of a file",
            inputSchema={
                "type": "object",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "File path"
                    }
                },
                "required": ["path"]
            }
        )
    ]

@app.call_tool()
async def call_tool(name: str, arguments: dict) -> str:
    """ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œ"""
    if name == "web_search":
        return await web_search(
            arguments["query"],
            arguments.get("max_results", 5)
        )
    elif name == "read_file":
        return await read_file(arguments["path"])
    else:
        raise ValueError(f"Unknown tool: {name}")

# ãƒ„ãƒ¼ãƒ«å®Ÿè£…

async def web_search(query: str, max_results: int) -> str:
    """Webæ¤œç´¢ã®å®Ÿè£…"""
    # å®Ÿéš›ã®æ¤œç´¢APIå‘¼ã³å‡ºã—
    # ä¾‹: SerpAPI, Brave Search API, etc.
    results = perform_search(query, max_results)

    return json.dumps(results, ensure_ascii=False, indent=2)

async def read_file(path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã®å®Ÿè£…"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        return f"Error reading file: {str(e)}"

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•

if __name__ == "__main__":
    stdio_server(app)
```

### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã®åˆ©ç”¨

```python
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def execute_tool_via_mcp(tool_name: str, params: dict) -> str:
    """MCPã‚’é€šã˜ã¦ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ"""
    server_params = StdioServerParameters(
        command="python",
        args=["mcp_tools.py"]
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—
            result = await session.call_tool(tool_name, params)

            return result.content[0].text
```

---

## ğŸ“š ã‚¹ãƒ†ãƒƒãƒ—5: RAGçµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

å¤–éƒ¨çŸ¥è­˜ã‚’å–ã‚Šè¾¼ã¿ã¾ã™ã€‚

### rag.py

```python
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

class RAGSystem:
    def __init__(self, persist_directory: str = "./chroma_db"):
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        self.vectorstore = None
        self.persist_directory = persist_directory

    def index_documents(self, docs_path: str):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
        loader = DirectoryLoader(docs_path, glob="**/*.md")
        documents = loader.load()

        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documents)

        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )

    def retrieve(self, query: str, k: int = 5) -> List[str]:
        """é–¢é€£æ–‡æ›¸ã‚’å–å¾—"""
        if not self.vectorstore:
            # æ°¸ç¶šåŒ–ã•ã‚ŒãŸDBã‹ã‚‰èª­ã¿è¾¼ã¿
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings
            )

        docs = self.vectorstore.similarity_search(query, k=k)

        return [doc.page_content for doc in docs]

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®çµ±åˆ

def build_context_with_rag(state: AgentState, rag: RAGSystem) -> str:
    """RAGã‚’ä½¿ã£ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰"""
    # é–¢é€£æ–‡æ›¸ã‚’å–å¾—
    relevant_docs = rag.retrieve(state["goal"], k=3)

    context = f"""
<goal>
{state['goal']}
</goal>

<relevant_knowledge>
{chr(10).join([f'<doc>{doc}</doc>' for doc in relevant_docs])}
</relevant_knowledge>

<known_facts>
{format_facts(state['known_facts'])}
</known_facts>

...
"""
    return context
```

---

## ğŸ¨ ã‚¹ãƒ†ãƒƒãƒ—6: UIã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### Streamlitãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```python
import streamlit as st
from agent import app, AgentState

st.title("Claude-Style Agent")

# åˆæœŸçŠ¶æ…‹
if "state" not in st.session_state:
    st.session_state.state = {
        "goal": "",
        "original_goal": "",
        "known_facts": [],
        "open_questions": [],
        "plan_steps": [],
        "current_step_index": 0,
        "messages": [],
        "observations": [],
        "iteration_count": 0,
        "status": "planning"
    }

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
goal = st.text_area("ç›®æ¨™ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", height=100)

if st.button("å®Ÿè¡Œ"):
    if goal:
        # åˆæœŸåŒ–
        st.session_state.state["goal"] = goal
        st.session_state.state["original_goal"] = goal

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
        with st.spinner("å®Ÿè¡Œä¸­..."):
            result = app.invoke(st.session_state.state)

        st.session_state.state = result

# é€²æ—è¡¨ç¤º
if st.session_state.state["plan_steps"]:
    st.subheader("è¨ˆç”»")
    for i, step in enumerate(st.session_state.state["plan_steps"]):
        status_icon = {
            "completed": "âœ…",
            "in_progress": "ğŸ”„",
            "pending": "â³",
            "failed": "âŒ"
        }.get(step["status"], "â“")

        st.write(f"{status_icon} **Step {i+1}**: {step['description']}")

# äº‹å®Ÿè¡¨ç¤º
if st.session_state.state["known_facts"]:
    st.subheader("åˆ¤æ˜ã—ãŸäº‹å®Ÿ")
    for fact in st.session_state.state["known_facts"]:
        st.write(f"- {fact['content']} ({fact['confidence']})")

# è¦³æ¸¬è¡¨ç¤º
if st.session_state.state["observations"]:
    st.subheader("è¦³æ¸¬çµæœ")
    for obs in st.session_state.state["observations"][-5:]:
        with st.expander(f"ğŸ“Š {obs['source']}"):
            st.code(obs['content'])
```

---

## ğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ†ã‚¹ãƒˆ

```python
import pytest
from agent import plan_node, act_node, AgentState

def test_plan_node():
    """è¨ˆç”»ãƒãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ"""
    initial_state: AgentState = {
        "goal": "Analyze sales data for Q1 2024",
        "original_goal": "Analyze sales data for Q1 2024",
        "known_facts": [],
        "open_questions": [],
        "plan_steps": [],
        "current_step_index": 0,
        "messages": [],
        "observations": [],
        "iteration_count": 0,
        "status": "planning"
    }

    result = plan_node(initial_state)

    assert result["status"] == "executing"
    assert len(result["plan_steps"]) > 0
    assert result["plan_steps"][0]["id"] == "1"

def test_context_builder():
    """Context Builderã®ãƒ†ã‚¹ãƒˆ"""
    builder = ContextBuilder(max_tokens=100000)

    state: AgentState = {
        "goal": "Test goal",
        "known_facts": [
            {
                "content": "Fact 1",
                "source": "user",
                "confidence": "high",
                "timestamp": time.time()
            }
        ],
        "open_questions": [],
        # ...
    }

    context = builder.build_planning_context(state)

    assert "<goal>Test goal</goal>" in context
    assert "<fact" in context
    assert builder.count_tokens(context) < 100000
```

---

## ğŸ“¦ å®Œå…¨ãªå®Ÿè£…ä¾‹

ã™ã¹ã¦ã‚’çµ±åˆã—ãŸã‚µãƒ³ãƒ—ãƒ«ï¼š

### main.py

```python
import asyncio
from agent import app, AgentState
from rag import RAGSystem
from context_builder import ContextBuilder

async def main():
    # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    rag = RAGSystem()
    rag.index_documents("./knowledge_base")

    # Context BuilderåˆæœŸåŒ–
    context_builder = ContextBuilder(max_tokens=180000)

    # åˆæœŸçŠ¶æ…‹
    initial_state: AgentState = {
        "goal": "Pythonã§ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã®æœ€é©ãªæ–¹æ³•ã‚’èª¿ã¹ã¦ã€ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ä½œæˆ",
        "original_goal": "Pythonã§ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã®æœ€é©ãªæ–¹æ³•ã‚’èª¿ã¹ã¦ã€ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ä½œæˆ",
        "known_facts": [],
        "open_questions": [],
        "plan_steps": [],
        "current_step_index": 0,
        "messages": [],
        "observations": [],
        "iteration_count": 0,
        "status": "planning"
    }

    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
    print("ğŸš€ Starting agent...")
    final_state = app.invoke(initial_state)

    # çµæœè¡¨ç¤º
    print("\nâœ… Task completed!")
    print(f"Status: {final_state['status']}")
    print(f"Iterations: {final_state['iteration_count']}")

    print("\nğŸ“‹ Completed Steps:")
    for step in final_state['plan_steps']:
        if step['status'] == 'completed':
            print(f"  âœ“ {step['description']}")
            if step['result']:
                print(f"    â†’ {step['result']}")

    print("\nğŸ“š Known Facts:")
    for fact in final_state['known_facts']:
        print(f"  - {fact['content']} ({fact['confidence']})")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸš€ å®Ÿè¡Œ

```bash
# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
python main.py

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•
streamlit run dashboard.py
```

---

## ğŸ“š å‚è€ƒè³‡æ–™

### å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Model Context Protocol](https://modelcontextprotocol.io/)
- [Anthropic API Docs](https://docs.anthropic.com/)

### é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [03-agent-architecture.md](./03-agent-architecture.md) - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ
- [05-multimodal-implementation.md](./05-multimodal-implementation.md) - ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ
- [06-practical-examples.md](./06-practical-examples.md) - å®Ÿè·µä¾‹

---

**æ¬¡**: [05-multimodal-implementation.md](./05-multimodal-implementation.md) - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»å‹•ç”»å‡¦ç†ã®å®Ÿè£…
