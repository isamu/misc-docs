# ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œã®å®Ÿè£…

## æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»å‹•ç”»ãƒ»ç”»åƒãªã©ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªå…¥åŠ›ã‚’å‡¦ç†ã™ã‚‹Claudeé¢¨ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã‚’è§£èª¬ã—ã¾ã™ã€‚

Claudeã¯ç”»åƒãƒ»PDFã‚’ç›´æ¥å‡¦ç†ã§ãã¾ã™ãŒã€OSSã§ã“ã‚Œã‚’å®Ÿè£…ã™ã‚‹å ´åˆã¯åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ã§ã™ã€‚

---

## ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†

### æ¨å¥¨ãƒ„ãƒ¼ãƒ«

| ãƒ„ãƒ¼ãƒ« | ç”¨é€” | ç‰¹å¾´ |
|-------|------|------|
| **Docling** | PDFè§£æ | Anthropicæ¨å¥¨ã€é«˜ç²¾åº¦ |
| **Unstructured** | å¤šå½¢å¼å¯¾å¿œ | PDF, DOCX, HTML, Markdown |
| **PyMuPDF** | è»½é‡PDFå‡¦ç† | é«˜é€Ÿã€ã‚·ãƒ³ãƒ—ãƒ« |

### Doclingã«ã‚ˆã‚‹å®Ÿè£…

Doclingï¼ˆ[GitHub](https://github.com/docling-project/docling)ï¼‰ã¯AnthropicãŒæ¨å¥¨ã™ã‚‹PDFå‡¦ç†ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

#### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install docling
```

#### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
from docling.document_converter import DocumentConverter

def extract_document(pdf_path: str) -> dict:
    """PDFã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    converter = DocumentConverter()

    # PDFã‚’å¤‰æ›
    result = converter.convert(pdf_path)

    # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    structured_data = {
        "title": result.document.title,
        "text": result.document.export_to_markdown(),
        "tables": extract_tables(result),
        "images": extract_images(result),
        "metadata": {
            "pages": result.document.page_count,
            "created_at": result.document.metadata.get("created_at")
        }
    }

    return structured_data

def extract_tables(result) -> list:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡º"""
    tables = []

    for page in result.document.pages:
        for table in page.tables:
            tables.append({
                "page": page.page_no,
                "data": table.export_to_dataframe().to_dict(),
                "caption": table.caption
            })

    return tables

def extract_images(result) -> list:
    """ç”»åƒã‚’æŠ½å‡º"""
    images = []

    for page in result.document.pages:
        for image in page.images:
            images.append({
                "page": page.page_no,
                "path": image.save(f"./images/page_{page.page_no}_{image.id}.png"),
                "caption": image.caption,
                "bbox": image.bbox
            })

    return images
```

### Unstructuredã«ã‚ˆã‚‹å¤šå½¢å¼å¯¾å¿œ

```python
from unstructured.partition.auto import partition

def process_any_document(file_path: str) -> dict:
    """ä»»æ„ã®å½¢å¼ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†"""
    # è‡ªå‹•çš„ã«å½¢å¼ã‚’åˆ¤å®šã—ã¦å‡¦ç†
    elements = partition(filename=file_path)

    # è¦ç´ ã‚’ç¨®é¡åˆ¥ã«åˆ†é¡
    categorized = {
        "title": [],
        "text": [],
        "tables": [],
        "lists": []
    }

    for element in elements:
        element_type = element.category

        if element_type == "Title":
            categorized["title"].append(element.text)
        elif element_type == "NarrativeText":
            categorized["text"].append(element.text)
        elif element_type == "Table":
            categorized["tables"].append(element.metadata.text_as_html)
        elif element_type == "ListItem":
            categorized["lists"].append(element.text)

    return categorized
```

### éšå±¤å‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—

å¤§ããªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯éšå±¤çš„ã«å‡¦ç†ã—ã¾ã™ã€‚

```python
class HierarchicalDocumentRetriever:
    def __init__(self):
        self.doc_summaries = {}  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã®è¦ç´„
        self.section_summaries = {}  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã®è¦ç´„
        self.chunks = {}  # è©³ç´°ãƒãƒ£ãƒ³ã‚¯

    async def index_document(self, doc_path: str):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’éšå±¤çš„ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã‚’æŠ½å‡º
        full_text = extract_document(doc_path)

        # 2. ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²
        sections = self.split_into_sections(full_text)

        # 3. å„ãƒ¬ãƒ™ãƒ«ã§è¦ç´„ç”Ÿæˆ
        doc_id = hash(doc_path)

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã®è¦ç´„
        self.doc_summaries[doc_id] = await self.summarize(
            full_text["text"],
            max_length=500
        )

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã®è¦ç´„
        for i, section in enumerate(sections):
            section_id = f"{doc_id}_section_{i}"

            self.section_summaries[section_id] = await self.summarize(
                section["content"],
                max_length=200
            )

            # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunks = self.split_into_chunks(section["content"], chunk_size=1000)

            for j, chunk in enumerate(chunks):
                chunk_id = f"{section_id}_chunk_{j}"
                self.chunks[chunk_id] = {
                    "content": chunk,
                    "doc_id": doc_id,
                    "section_id": section_id,
                    "metadata": {
                        "section_title": section["title"],
                        "page": section["page"]
                    }
                }

    async def retrieve(self, query: str, detail_level: str = "auto") -> str:
        """ã‚¯ã‚¨ãƒªã«å¿œã˜ã¦é©åˆ‡ãªè©³ç´°åº¦ã§å–å¾—"""

        if detail_level == "overview":
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®è¦ç´„ã®ã¿
            relevant_docs = self.find_relevant_docs(query)
            return "\n\n".join([
                self.doc_summaries[doc_id]
                for doc_id in relevant_docs
            ])

        elif detail_level == "section":
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«
            relevant_sections = self.find_relevant_sections(query)
            return "\n\n".join([
                self.section_summaries[sec_id]
                for sec_id in relevant_sections
            ])

        elif detail_level == "detailed":
            # ãƒãƒ£ãƒ³ã‚¯ãƒ¬ãƒ™ãƒ«
            relevant_chunks = self.find_relevant_chunks(query)
            return "\n\n".join([
                self.chunks[chunk_id]["content"]
                for chunk_id in relevant_chunks
            ])

        else:  # auto
            # ã‚¯ã‚¨ãƒªã®è¤‡é›‘ã•ã§åˆ¤æ–­
            if self.is_simple_query(query):
                return await self.retrieve(query, "overview")
            elif self.needs_details(query):
                return await self.retrieve(query, "detailed")
            else:
                return await self.retrieve(query, "section")

    def split_into_sections(self, doc: dict) -> list:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²"""
        # Markdownè¦‹å‡ºã—ãƒ™ãƒ¼ã‚¹ã§åˆ†å‰²
        text = doc["text"]
        sections = []

        current_section = {"title": "", "content": "", "page": 0}

        for line in text.split("\n"):
            if line.startswith("# "):
                # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                if current_section["content"]:
                    sections.append(current_section)

                current_section = {
                    "title": line[2:].strip(),
                    "content": "",
                    "page": 0  # ãƒšãƒ¼ã‚¸ç•ªå·ã¯åˆ¥é€”å–å¾—
                }
            else:
                current_section["content"] += line + "\n"

        if current_section["content"]:
            sections.append(current_section)

        return sections
```

---

## ğŸ¥ å‹•ç”»å‡¦ç†

### æ¨å¥¨ãƒ„ãƒ¼ãƒ«

| ãƒ„ãƒ¼ãƒ« | ç”¨é€” | ç‰¹å¾´ |
|-------|------|------|
| **LLaVA-NeXT-Video** | å‹•ç”»ç†è§£ | é«˜ç²¾åº¦ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å¯¾å¿œ |
| **Qwen2-VL** | ãƒ“ã‚¸ãƒ§ãƒ³è¨€èªãƒ¢ãƒ‡ãƒ« | å¤šè¨€èªå¯¾å¿œ |
| **VideoLLaMA** | å‹•ç”»Q&A | è»½é‡ |

### LLaVA-NeXT-Videoã«ã‚ˆã‚‹å®Ÿè£…

#### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install llava-next
pip install av  # å‹•ç”»å‡¦ç†
```

#### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
from llava.model.builder import load_pretrained_model
from llava.mm_utils import process_video
import av

class VideoProcessor:
    def __init__(self, model_path: str = "lmms-lab/LLaVA-NeXT-Video-7B"):
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        self.tokenizer, self.model, self.image_processor, self.context_len = \
            load_pretrained_model(model_path)

    def extract_frames(
        self,
        video_path: str,
        num_frames: int = 8
    ) -> list:
        """å‡ç­‰ã«ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º"""
        container = av.open(video_path)
        video_stream = container.streams.video[0]

        total_frames = video_stream.frames
        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)

        frames = []
        for i, frame in enumerate(container.decode(video=0)):
            if i in indices:
                frames.append(frame.to_ndarray(format='rgb24'))

        return frames

    async def understand_video(
        self,
        video_path: str,
        query: str
    ) -> dict:
        """å‹•ç”»ã®å†…å®¹ã‚’ç†è§£"""
        # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
        frames = self.extract_frames(video_path, num_frames=8)

        # å‡¦ç†
        video_tensor = process_video(frames, self.image_processor)

        # è³ªå•
        prompt = f"USER: <video>\n{query}\nASSISTANT:"

        # æ¨è«–
        input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids

        output_ids = self.model.generate(
            input_ids,
            images=video_tensor,
            max_new_tokens=512
        )

        response = self.tokenizer.decode(
            output_ids[0],
            skip_special_tokens=True
        )

        return {
            "query": query,
            "response": response,
            "num_frames": len(frames)
        }

    async def extract_timeline(
        self,
        video_path: str
    ) -> list:
        """ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’æŠ½å‡ºï¼ˆã‚·ãƒ¼ãƒ³å¤‰åŒ–æ¤œå‡ºï¼‰"""
        frames = self.extract_frames(video_path, num_frames=32)

        timeline = []

        # å„ãƒ•ãƒ¬ãƒ¼ãƒ ã§å†…å®¹ã‚’è¦ç´„
        for i, frame in enumerate(frames):
            timestamp = (i / len(frames)) * self.get_duration(video_path)

            description = await self.describe_frame(frame)

            timeline.append({
                "timestamp": timestamp,
                "description": description,
                "frame_index": i
            })

        return timeline

    async def describe_frame(self, frame) -> str:
        """1ãƒ•ãƒ¬ãƒ¼ãƒ ã®å†…å®¹ã‚’èª¬æ˜"""
        # ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†
        frame_tensor = self.image_processor(frame)

        prompt = "USER: <image>\nDescribe what you see in this image briefly.\nASSISTANT:"

        input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids

        output_ids = self.model.generate(
            input_ids,
            images=frame_tensor,
            max_new_tokens=100
        )

        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
```

### ã‚¿ã‚¤ãƒ ã‚³ãƒ¼ãƒ‰ä»˜ãå›ç­”

```python
class VideoQA:
    def __init__(self, video_processor: VideoProcessor):
        self.processor = video_processor
        self.timeline_cache = {}

    async def answer_with_timestamp(
        self,
        video_path: str,
        question: str
    ) -> dict:
        """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§å›ç­”"""

        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
        if video_path not in self.timeline_cache:
            self.timeline_cache[video_path] = \
                await self.processor.extract_timeline(video_path)

        timeline = self.timeline_cache[video_path]

        # è³ªå•ã«é–¢é€£ã™ã‚‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç‰¹å®š
        relevant_moments = await self.find_relevant_moments(
            question,
            timeline
        )

        # è©³ç´°ãªå›ç­”ã‚’ç”Ÿæˆ
        detailed_answer = await self.processor.understand_video(
            video_path,
            question
        )

        return {
            "answer": detailed_answer["response"],
            "timestamps": [
                {
                    "time": moment["timestamp"],
                    "description": moment["description"]
                }
                for moment in relevant_moments
            ],
            "sources": [
                f"Video at {self.format_timestamp(m['timestamp'])}: {m['description']}"
                for m in relevant_moments
            ]
        }

    async def find_relevant_moments(
        self,
        query: str,
        timeline: list
    ) -> list:
        """è³ªå•ã«é–¢é€£ã™ã‚‹ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ç‰¹å®š"""
        # ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ã§æ¤œç´¢
        query_embedding = self.embed(query)

        scored_moments = []
        for moment in timeline:
            moment_embedding = self.embed(moment["description"])
            similarity = cosine_similarity(query_embedding, moment_embedding)

            scored_moments.append({
                **moment,
                "relevance": similarity
            })

        # ä¸Šä½3ä»¶ã‚’è¿”ã™
        scored_moments.sort(key=lambda x: x["relevance"], reverse=True)
        return scored_moments[:3]

    def format_timestamp(self, seconds: float) -> str:
        """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"
```

---

## ğŸ–¼ï¸ ç”»åƒå‡¦ç†

Claudeã¯ç”»åƒã‚’ç›´æ¥å‡¦ç†ã§ãã¾ã™ãŒã€OSSã®å ´åˆã¯Vision LLMã‚’ä½¿ã„ã¾ã™ã€‚

### Qwen2-VLã«ã‚ˆã‚‹å®Ÿè£…

```python
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image

class ImageUnderstanding:
    def __init__(self, model_name: str = "Qwen/Qwen2-VL-7B-Instruct"):
        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.processor = AutoProcessor.from_pretrained(model_name)

    async def analyze_image(
        self,
        image_path: str,
        query: str = "Describe this image in detail."
    ) -> str:
        """ç”»åƒã‚’åˆ†æ"""
        image = Image.open(image_path)

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": query}
                ]
            }
        ]

        # æ¨è«–
        text = self.processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.processor(
            text=[text],
            images=[image],
            return_tensors="pt"
        ).to(self.model.device)

        output_ids = self.model.generate(**inputs, max_new_tokens=512)

        response = self.processor.batch_decode(
            output_ids,
            skip_special_tokens=True
        )[0]

        return response

    async def extract_text_from_image(self, image_path: str) -> str:
        """ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆOCRï¼‰"""
        return await self.analyze_image(
            image_path,
            "Extract all text visible in this image."
        )

    async def answer_visual_question(
        self,
        image_path: str,
        question: str
    ) -> str:
        """ç”»åƒã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”"""
        return await self.analyze_image(image_path, question)
```

---

## ğŸ”— ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«RAG

ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±åˆã—ã¾ã™ã€‚

```python
class MultimodalRAG:
    def __init__(self):
        self.doc_processor = DocumentConverter()
        self.video_processor = VideoProcessor()
        self.image_processor = ImageUnderstanding()
        self.vector_store = ChromaDB()

    async def index_multimodal_content(self, content_dir: str):
        """ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        for file_path in Path(content_dir).rglob("*"):
            if file_path.suffix.lower() in [".pdf", ".docx", ".md"]:
                await self.index_document(file_path)

            elif file_path.suffix.lower() in [".mp4", ".avi", ".mov"]:
                await self.index_video(file_path)

            elif file_path.suffix.lower() in [".png", ".jpg", ".jpeg"]:
                await self.index_image(file_path)

    async def index_document(self, doc_path: str):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        data = extract_document(doc_path)

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–
        chunks = self.chunk_text(data["text"])

        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜
        for i, chunk in enumerate(chunks):
            self.vector_store.add(
                id=f"{doc_path}_chunk_{i}",
                text=chunk,
                metadata={
                    "source": doc_path,
                    "type": "document",
                    "chunk_index": i
                }
            )

    async def index_video(self, video_path: str):
        """å‹•ç”»ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æŠ½å‡º
        timeline = await self.video_processor.extract_timeline(video_path)

        # å„ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        for moment in timeline:
            self.vector_store.add(
                id=f"{video_path}_t_{moment['timestamp']}",
                text=moment["description"],
                metadata={
                    "source": video_path,
                    "type": "video",
                    "timestamp": moment["timestamp"]
                }
            )

    async def index_image(self, image_path: str):
        """ç”»åƒã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
        # ç”»åƒã®èª¬æ˜ã‚’ç”Ÿæˆ
        description = await self.image_processor.analyze_image(image_path)

        self.vector_store.add(
            id=image_path,
            text=description,
            metadata={
                "source": image_path,
                "type": "image"
            }
        )

    async def retrieve(self, query: str, k: int = 5) -> list:
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
        results = self.vector_store.search(query, k=k)

        enriched_results = []

        for result in results:
            metadata = result["metadata"]

            if metadata["type"] == "document":
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯
                enriched_results.append({
                    "content": result["text"],
                    "source": metadata["source"],
                    "type": "document"
                })

            elif metadata["type"] == "video":
                # å‹•ç”»ã®è©²å½“ç®‡æ‰€
                enriched_results.append({
                    "content": result["text"],
                    "source": metadata["source"],
                    "type": "video",
                    "timestamp": metadata["timestamp"],
                    "reference": f"{metadata['source']} at {self.format_time(metadata['timestamp'])}"
                })

            elif metadata["type"] == "image":
                # ç”»åƒ
                enriched_results.append({
                    "content": result["text"],
                    "source": metadata["source"],
                    "type": "image",
                    "reference": f"Image: {metadata['source']}"
                })

        return enriched_results

    async def answer_with_sources(self, query: str) -> dict:
        """ã‚½ãƒ¼ã‚¹ä»˜ãã§å›ç­”"""
        # é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        sources = await self.retrieve(query, k=5)

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context = self.build_context(sources)

        # LLMã§å›ç­”ç”Ÿæˆ
        answer = await self.generate_answer(query, context)

        return {
            "answer": answer,
            "sources": [
                {
                    "type": s["type"],
                    "reference": s.get("reference", s["source"]),
                    "excerpt": s["content"][:200]
                }
                for s in sources
            ]
        }

    def build_context(self, sources: list) -> str:
        """ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        context_parts = []

        for source in sources:
            if source["type"] == "document":
                context_parts.append(f"""
<document_excerpt source="{source['source']}">
{source['content']}
</document_excerpt>
""")

            elif source["type"] == "video":
                context_parts.append(f"""
<video_moment source="{source['source']}" timestamp="{source['timestamp']}">
{source['content']}
</video_moment>
""")

            elif source["type"] == "image":
                context_parts.append(f"""
<image_description source="{source['source']}">
{source['content']}
</image_description>
""")

        return "\n".join(context_parts)
```

---

## ğŸ’° ã‚³ã‚¹ãƒˆæœ€é©åŒ–

ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å‡¦ç†ã¯ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŸã‚ã€æœ€é©åŒ–ãŒé‡è¦ã§ã™ã€‚

### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

```python
class MultimodalCache:
    def __init__(self, cache_dir: str = "./cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def get_cache_key(self, file_path: str, operation: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        file_hash = hashlib.md5(Path(file_path).read_bytes()).hexdigest()
        return f"{operation}_{file_hash}"

    async def get_or_compute(
        self,
        file_path: str,
        operation: str,
        compute_fn
    ):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°è¿”ã™ã€ãªã‘ã‚Œã°è¨ˆç®—"""
        cache_key = self.get_cache_key(file_path, operation)
        cache_path = self.cache_dir / f"{cache_key}.json"

        if cache_path.exists():
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
            with open(cache_path, 'r') as f:
                return json.load(f)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã€è¨ˆç®—å®Ÿè¡Œ
        result = await compute_fn(file_path)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        with open(cache_path, 'w') as f:
            json.dump(result, f)

        return result

# ä½¿ç”¨ä¾‹

cache = MultimodalCache()

# å‹•ç”»å‡¦ç†ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ï¼‰
timeline = await cache.get_or_compute(
    video_path,
    "extract_timeline",
    video_processor.extract_timeline
)
```

### æ®µéšçš„å‡¦ç†

```python
async def process_video_efficiently(video_path: str, query: str):
    """æ®µéšçš„ã«å‡¦ç†ã—ã¦ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›"""

    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®ã¿ï¼ˆä½ã‚³ã‚¹ãƒˆï¼‰
    timeline = await video_processor.extract_timeline(video_path)

    # ã‚¹ãƒ†ãƒƒãƒ—2: é–¢é€£éƒ¨åˆ†ã‚’ç‰¹å®š
    relevant_moments = find_relevant_moments(query, timeline)

    if not relevant_moments:
        return "å‹•ç”»ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"

    # ã‚¹ãƒ†ãƒƒãƒ—3: é–¢é€£éƒ¨åˆ†ã®ã¿è©³ç´°å‡¦ç†ï¼ˆé«˜ã‚³ã‚¹ãƒˆï¼‰
    detailed_analyses = []

    for moment in relevant_moments[:3]:  # ä¸Šä½3ä»¶ã®ã¿
        analysis = await video_processor.analyze_frame(
            video_path,
            moment["frame_index"],
            query
        )
        detailed_analyses.append(analysis)

    return synthesize_answer(query, detailed_analyses)
```

---

## ğŸ“š å‚è€ƒè³‡æ–™

### å…¬å¼ãƒªã‚½ãƒ¼ã‚¹

- [Docling GitHub](https://github.com/docling-project/docling)
- [LLaVA-NeXT](https://llava-vl.github.io/blog/2024-01-30-llava-next/)
- [Qwen2-VL](https://github.com/QwenLM/Qwen2-VL)

### é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [04-implementation-guide.md](./04-implementation-guide.md) - åŸºæœ¬å®Ÿè£…
- [06-practical-examples.md](./06-practical-examples.md) - å®Ÿè·µä¾‹

---

**æ¬¡**: [06-practical-examples.md](./06-practical-examples.md) - å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹
